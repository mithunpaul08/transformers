{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "id": "21lGjujJKkP8",
    "outputId": "fcb6f917-0f2a-4364-873a-cfe31e70d3e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368.0
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Finetuning the library models for sequence classification on GLUE (Bert, XLM, XLNet, RoBERTa, Albert, XLM-RoBERTa).\"\"\"\n",
    "CONFIG_FILE_TO_TEST_WITH=\"config_for_colab_visualization.py\"\n",
    "import configparser\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from typing import  Callable, Dict, List, Optional, Tuple\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import numpy as np\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from tqdm.auto import tqdm, trange\n",
    "import os\n",
    "\n",
    "\n",
    "from bertviz import head_view\n",
    "os.chdir(\"/content/gdrive/My Drive/colab_fall2020/transformers/src\")\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator, collate_batch_parallel_datasets\n",
    "from transformers.file_utils import is_apex_available, is_torch_tpu_available\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers.trainer_utils import (\n",
    "    PREFIX_CHECKPOINT_DIR,\n",
    "    EvalPrediction,\n",
    "    PredictionOutput,\n",
    "    TrainOutput,\n",
    "    is_wandb_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "\n",
    "    glue_compute_metrics,\n",
    "    glue_output_modes,\n",
    "    glue_tasks_num_labels,\n",
    "    set_seed,\n",
    ")\n",
    "os.chdir(\"/content/gdrive/My Drive/colab_fall2020/transformers/examples/text-classification\")\n",
    "import git\n",
    "\n",
    "\n",
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, Optional\n",
    "import git\n",
    "import numpy as np\n",
    "\n",
    "import wget\n",
    "import torch\n",
    "\n",
    "import wget\n",
    "import torch\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "\n",
    "if is_torch_tpu_available():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.debug.metrics as met\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "    _has_tensorboard = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        from tensorboardX import SummaryWriter\n",
    "\n",
    "        _has_tensorboard = True\n",
    "    except ImportError:\n",
    "        _has_tensorboard = False\n",
    "\n",
    "def is_tensorboard_available():\n",
    "    return _has_tensorboard\n",
    "\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class StudentTeacherTrainer:\n",
    "    \"\"\"\n",
    "    Trainer is a simple but feature-complete training and eval loop for PyTorch,\n",
    "\n",
    "    optimized for ðŸ¤— Transformers.\n",
    "\n",
    "    Args:\n",
    "        model (:class:`~transformers.PreTrainedModel`):\n",
    "            The model to train, evaluate or use for predictions.\n",
    "        args (:class:`~transformers.TrainingArguments`):\n",
    "            The arguments to tweak training.\n",
    "        data_collator (:obj:`DataCollator`, `optional`, defaults to :func:`~transformers.default_data_collator`):\n",
    "            The function to use to from a batch from a list of elements of :obj:`train_dataset` or\n",
    "            :obj:`dev_dataset`.\n",
    "        train_dataset (:obj:`torch.utils.data.dataset.Dataset`, `optional`):\n",
    "            The dataset to use for training.\n",
    "        eval_dataset (:obj:`torch.utils.data.dataset.Dataset`, `optional`):\n",
    "            The dataset to use for evaluation.\n",
    "        compute_metrics (:obj:`Callable[[EvalPrediction], Dict]`, `optional`):\n",
    "            The function that will be used to compute metrics at evaluation. Must take a\n",
    "            :class:`~transformers.EvalPrediction` and return a dictionary string to metric values.\n",
    "        prediction_loss_only (:obj:`bool`, `optional`, defaults to `False`):\n",
    "            When performing evaluation and predictions, only returns the loss.\n",
    "        tb_writer (:obj:`SummaryWriter`, `optional`):\n",
    "            Object to write to TensorBoard.\n",
    "        optimizers (:obj:`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR`, `optional`):\n",
    "            A tuple containing the optimizer and the scheduler to use. Will default to an instance of\n",
    "            :class:`~transformers.AdamW` on your model and a scheduler given by\n",
    "            :func:`~transformers.get_linear_schedule_with_warmup` controlled by :obj:`args`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    models: {}\n",
    "    args: TrainingArguments\n",
    "    data_collator: DataCollator\n",
    "    train_dataset: Optional[Dataset]\n",
    "    eval_dataset: Optional[Dataset]\n",
    "    test_dataset = Optional[Dataset]\n",
    "    test_compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None\n",
    "    eval_compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None\n",
    "    compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None\n",
    "    prediction_loss_only: bool\n",
    "    tb_writer: Optional[\"SummaryWriter\"] = None\n",
    "    optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = None\n",
    "    global_step: Optional[int] = None\n",
    "    epoch: Optional[float] = None\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer_delex,\n",
    "            tokenizer_lex,\n",
    "            models: {},\n",
    "            args: TrainingArguments,\n",
    "            train_datasets: {},\n",
    "            eval_dataset: Optional[Dataset] = None,\n",
    "            test_dataset: Optional[Dataset] = None,\n",
    "            test_compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
    "            eval_compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
    "            compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
    "            data_collator: Optional[DataCollator] = None,\n",
    "            prediction_loss_only=False,\n",
    "            tb_writer: Optional[\"SummaryWriter\"] = None,\n",
    "            optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None)\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Trainer is a simple but feature-complete training and eval loop for PyTorch,\n",
    "        optimized for Transformers.\n",
    "        Args:\n",
    "            prediction_loss_only:\n",
    "                (Optional) in evaluation and prediction, only return the loss\n",
    "        \"\"\"\n",
    "        self.lex_teacher_model = models.get(\"teacher\").to(args.device)\n",
    "        self.delex_student_model = models.get(\"student\").to(args.device)\n",
    "\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.lex_tokenizer = tokenizer_lex\n",
    "        self.delex_tokenizer = tokenizer_delex\n",
    "\n",
    "        ###for fnc score evaluation\n",
    "        self.test_dataset = test_dataset\n",
    "        self.test_compute_metrics = test_compute_metrics\n",
    "        self.eval_compute_metrics = eval_compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        # even though we train two models using student teacher architecture we weill only use the student model to do evaluation on fnc-dev mod2 dataset\n",
    "        self.model = None\n",
    "        self.args = args\n",
    "        self.default_data_collator = default_data_collator\n",
    "        self.data_collator = collate_batch_parallel_datasets\n",
    "        self.train_dataset_combined = train_datasets.get(\"combined\")\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.compute_metrics = None\n",
    "        self.prediction_loss_only = prediction_loss_only\n",
    "        self.optimizer, self.lr_scheduler = optimizers\n",
    "        if tb_writer is not None:\n",
    "            self.tb_writer = tb_writer\n",
    "        elif is_tensorboard_available() and self.is_world_master():\n",
    "            self.tb_writer = SummaryWriter(log_dir=self.args.logging_dir)\n",
    "        if not is_tensorboard_available():\n",
    "            logger.warning(\n",
    "                \"You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\"\n",
    "            )\n",
    "        if is_wandb_available():\n",
    "            self._setup_wandb()\n",
    "        else:\n",
    "            logger.info(\n",
    "                \"You are instantiating a Trainer but W&B is not installed. To use wandb logging, \"\n",
    "                \"run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\"\n",
    "            )\n",
    "        set_seed(self.args.seed)\n",
    "        # Create output directory if needed\n",
    "        if self.is_world_master():\n",
    "            os.makedirs(self.args.output_dir, exist_ok=True)\n",
    "        if is_torch_tpu_available():\n",
    "            # Set an xla_device flag on the model's config.\n",
    "            # We'll find a more elegant and not need to do this in the future.\n",
    "            self.model.config.xla_device = True\n",
    "\n",
    "        if not callable(self.data_collator) and callable(getattr(self.data_collator, \"collate_batch\", None)):\n",
    "            self.data_collator = self.data_collator.collate_batch\n",
    "            warnings.warn(\n",
    "                (\n",
    "                        \"The `data_collator` should now be a simple callable (function, class with `__call__`), classes \"\n",
    "                        + \"with a `collate_batch` are deprecated and won't be supported in a future version.\"\n",
    "                ),\n",
    "                FutureWarning,\n",
    "            )\n",
    "\n",
    "    def write_predictions_to_disk(self, plain_text, gold_labels, predictions_logits, file_to_write_predictions,\n",
    "                                  test_dataset):\n",
    "        predictions_argmaxes = np.argmax(predictions_logits, axis=1)\n",
    "        sf = torch.nn.Softmax(dim=1)\n",
    "        predictions_softmax = sf(torch.FloatTensor(predictions_logits))\n",
    "        if self.is_world_master():\n",
    "            with open(file_to_write_predictions, \"w\") as writer:\n",
    "                logger.info(f\"***** (Going to write Test results to disk at {file_to_write_predictions} *****\")\n",
    "                writer.write(\"index\\t gold\\tprediction_logits\\t prediction_label\\tplain_text\\n\")\n",
    "                for index, (gold, pred_sf, pred_argmax, plain) in enumerate(\n",
    "                        zip(gold_labels, predictions_softmax, predictions_argmaxes, plain_text)):\n",
    "                    gold_string = test_dataset.get_labels()[gold]\n",
    "                    pred_string = test_dataset.get_labels()[pred_argmax]\n",
    "                    writer.write(\n",
    "                        \"%d\\t%s\\t%s\\t%s\\t%s\\n\" % (index, gold_string, str(pred_sf.tolist()), pred_string, plain))\n",
    "\n",
    "    def predict_given_trained_model(self, model, test_dataset):\n",
    "        predictions_argmaxes = self.predict(test_dataset, model).predictions_argmaxes\n",
    "\n",
    "    def predict(self, test_dataset: Dataset, model_to_test_with) -> PredictionOutput:\n",
    "        \"\"\"\n",
    "        Run prediction and returns predictions and potential metrics.\n",
    "\n",
    "        Depending on the dataset and your use case, your test dataset may contain labels.\n",
    "        In that case, this method will also return metrics, like in :obj:`evaluate()`.\n",
    "\n",
    "        Args:\n",
    "            test_dataset (:obj:`Dataset`):\n",
    "                Dataset to run the predictions on.\n",
    "        Returns:\n",
    "            `NamedTuple`:\n",
    "            predictions (:obj:`np.ndarray`):\n",
    "                The predictions on :obj:`test_dataset`.\n",
    "            label_ids (:obj:`np.ndarray`, `optional`):\n",
    "                The labels (if the dataset contained some).\n",
    "            metrics (:obj:`Dict[str, float]`, `optional`):\n",
    "                The potential dictionary of metrics (if the dataset contained labels).\n",
    "        \"\"\"\n",
    "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
    "\n",
    "        return self.prediction_loop(test_dataloader, model_to_test_with, description=\"Prediction\")\n",
    "\n",
    "    def evaluate(self, model_to_test_with, eval_dataset: Optional[Dataset] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Run evaluation and returns metrics.\n",
    "\n",
    "        The calling script will be responsible for providing a method to compute metrics, as they are\n",
    "        task-dependent (pass it to the init :obj:`eval_compute_metrics` argument).\n",
    "\n",
    "        You can also subclass and override this method to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            eval_dataset (:obj:`Dataset`, `optional`):\n",
    "                Pass a dataset if you wish to override :obj:`self.eval_dataset`.\n",
    "        Returns:\n",
    "            A dictionary containing the evaluation loss and the potential metrics computed from the predictions.\n",
    "        \"\"\"\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "\n",
    "        output, plain_text = self.prediction_loop(eval_dataloader, model_to_test_with, description=\"Evaluation\")\n",
    "        gold_labels = output.label_ids\n",
    "        predictions = output.predictions\n",
    "\n",
    "        self.log(output.metrics)\n",
    "        if self.args.tpu_metrics_debug or self.args.debug:\n",
    "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            xm.master_print(met.metrics_report())\n",
    "\n",
    "        return output.metrics, plain_text, gold_labels, predictions\n",
    "\n",
    "    def _prepare_inputs(\n",
    "            self, inputs: Dict[str, torch.Tensor], model: nn.Module\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare :obj:`inputs` before feeding them to the model, converting them to tensors if they are not already and\n",
    "        handling potential state.\n",
    "        \"\"\"\n",
    "        for k, v in inputs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(self.args.device)\n",
    "\n",
    "        if self.args.past_index >= 0 and self._past is not None:\n",
    "            inputs[\"mems\"] = self._past\n",
    "        # Our model outputs do not work with DataParallel, so forcing return tuple.\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            inputs[\"return_tuple\"] = True\n",
    "        return inputs\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        if self.train_dataset_combined is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        if is_torch_tpu_available():\n",
    "            train_sampler = get_tpu_sampler(self.train_dataset_combined)\n",
    "        else:\n",
    "            train_sampler = (\n",
    "                RandomSampler(self.train_dataset_combined)\n",
    "                if self.args.local_rank == -1\n",
    "                else DistributedSampler(self.train_dataset_combined)\n",
    "            )\n",
    "        data_loader = DataLoader(\n",
    "            self.train_dataset_combined,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=self.data_collator.collate_batch,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    def get_train_dataloader_two_parallel_datasets(self) -> DataLoader:\n",
    "        if self.train_dataset_combined is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        if is_torch_tpu_available():\n",
    "            train_sampler = get_tpu_sampler(self.train_dataset_combined)\n",
    "        else:\n",
    "            train_sampler = (\n",
    "                RandomSampler(self.train_dataset_combined)\n",
    "                if self.args.local_rank == -1\n",
    "                else DistributedSampler(self.train_dataset_combined)\n",
    "            )\n",
    "        data_loader = DataLoader(\n",
    "            self.train_dataset_combined,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n",
    "        if eval_dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "\n",
    "        if is_torch_tpu_available():\n",
    "            sampler = SequentialDistributedSampler(\n",
    "                eval_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()\n",
    "            )\n",
    "        elif self.args.local_rank != -1:\n",
    "            sampler = SequentialDistributedSampler(eval_dataset)\n",
    "        else:\n",
    "            sampler = SequentialSampler(eval_dataset)\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            eval_dataset,\n",
    "            sampler=sampler,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            collate_fn=self.default_data_collator\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def get_test_dataloader(self, test_dataset: Dataset) -> DataLoader:\n",
    "        # We use the same batch_size as for eval.\n",
    "        if is_torch_tpu_available():\n",
    "            sampler = SequentialDistributedSampler(\n",
    "                test_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()\n",
    "            )\n",
    "        elif self.args.local_rank != -1:\n",
    "            sampler = SequentialDistributedSampler(test_dataset)\n",
    "        else:\n",
    "            sampler = SequentialSampler(test_dataset)\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler=sampler,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            collate_fn=self.default_data_collator,\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def get_optimizers_for_student_teacher(\n",
    "            self, num_training_steps: int) -> Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]:\n",
    "        \"\"\"\n",
    "        Setup the optimizer and the learning rate scheduler.\n",
    "        We provide a reasonable default that works well.\n",
    "        If you want to use something else, you can pass a tuple in the Trainer's init,\n",
    "        or override this method in a subclass.\n",
    "        update: will combine parameters\n",
    "        \"\"\"\n",
    "        if self.optimizer is not None:\n",
    "            return self.optimizers\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.lex_teacher_model.named_parameters() if\n",
    "                           not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.lex_teacher_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.delex_student_model.named_parameters() if\n",
    "                           not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.delex_student_model.named_parameters() if\n",
    "                           any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps\n",
    "        )\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def get_optimizer(\n",
    "            self, model, num_training_steps: int) -> Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]:\n",
    "        \"\"\"\n",
    "        Setup the optimizer and the learning rate scheduler.\n",
    "        We provide a reasonable default that works well.\n",
    "        If you want to use something else, you can pass a tuple in the Trainer's init,\n",
    "        or override this method in a subclass.\n",
    "        update: will combine parameters\n",
    "        \"\"\"\n",
    "        if self.optimizer is not None:\n",
    "            return self.optimizers\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps\n",
    "        )\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def _setup_wandb(self):\n",
    "        \"\"\"\n",
    "        Setup the optional Weights & Biases (`wandb`) integration.\n",
    "        One can override this method to customize the setup if needed.  Find more information at https://docs.wandb.com/huggingface\n",
    "        You can also override the following environment variables:\n",
    "        Environment:\n",
    "            WANDB_WATCH:\n",
    "                (Optional, [\"gradients\", \"all\", \"false\"]) \"gradients\" by default, set to \"false\" to disable gradient logging\n",
    "                or \"all\" to log gradients and parameters\n",
    "            WANDB_PROJECT:\n",
    "                (Optional): str - \"huggingface\" by default, set this to a custom string to store results in a different project\n",
    "            WANDB_DISABLED:\n",
    "                (Optional): boolean - defaults to false, set to \"true\" to disable wandb entirely\n",
    "        \"\"\"\n",
    "        logger.info('Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"')\n",
    "        wandb.init(project=os.getenv(\"WANDB_PROJECT\", \"huggingface\"), config=vars(self.args))\n",
    "        # keep track of model topology and gradients\n",
    "        if os.getenv(\"WANDB_WATCH\") != \"false\":\n",
    "            wandb.watch(\n",
    "                self.lex_teacher_model, log=os.getenv(\"WANDB_WATCH\", \"gradients\"),\n",
    "                log_freq=max(100, self.args.logging_steps)\n",
    "            )\n",
    "            wandb.watch(\n",
    "                self.delex_student_model, log=os.getenv(\"WANDB_WATCH\", \"gradients\"),\n",
    "                log_freq=max(100, self.args.logging_steps)\n",
    "            )\n",
    "\n",
    "    def num_examples(self, dataloader: DataLoader) -> int:\n",
    "        \"\"\"\n",
    "        Helper to get num of examples from a DataLoader, by accessing its Dataset.\n",
    "        \"\"\"\n",
    "        return len(dataloader.dataset)\n",
    "\n",
    "    def get_plaintext_given_dataloader(self):\n",
    "        pass\n",
    "        # return eval_dataloader.dataset.features\n",
    "\n",
    "    def evaluate_on_test_partition(self, model_to_test_with, test_dataset: Optional[Dataset] = None, ) -> Dict[\n",
    "        str, float]:\n",
    "        \"\"\"\n",
    "        Run evaluation and returns metrics.\n",
    "        The calling script will be responsible for providing a method to compute metrics, as they are\n",
    "        task-dependent (pass it to the init :obj:`eval_compute_metrics` argument).\n",
    "        You can also subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            test_dataset (:obj:`Dataset`, `optional`):\n",
    "                Pass a dataset if you wish to override :obj:`self.eval_dataset`.\n",
    "        Returns:\n",
    "            A dictionary containing the evaluation loss and the potential metrics computed from the predictions.\n",
    "        \"\"\"\n",
    "        eval_dataloader = self.get_test_dataloader(test_dataset)\n",
    "\n",
    "        output, plain_text = self.prediction_loop(eval_dataloader, model_to_test_with, description=\"Evaluation\")\n",
    "        gold_labels = output.label_ids\n",
    "        predictions = output.predictions\n",
    "\n",
    "        self.log(output.metrics)\n",
    "\n",
    "        if self.args.tpu_metrics_debug or self.args.debug:\n",
    "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            xm.master_print(met.metrics_report())\n",
    "        return output.metrics, plain_text, gold_labels, predictions\n",
    "\n",
    "    def _rotate_checkpoints(self, use_mtime=False) -> None:\n",
    "        if self.args.save_total_limit is None or self.args.save_total_limit <= 0:\n",
    "            return\n",
    "\n",
    "        # Check if we should delete older checkpoint(s)\n",
    "        checkpoints_sorted = self._sorted_checkpoints(use_mtime=use_mtime)\n",
    "        if len(checkpoints_sorted) <= self.args.save_total_limit:\n",
    "            return\n",
    "\n",
    "        number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - self.args.save_total_limit)\n",
    "        checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "        for checkpoint in checkpoints_to_be_deleted:\n",
    "            logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "            shutil.rmtree(checkpoint)\n",
    "\n",
    "    def log(self, logs: Dict[str, float], iterator: Optional[tqdm] = None) -> None:\n",
    "        \"\"\"\n",
    "        Log :obj:`logs` on the various objects watching training.\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            logs (:obj:`Dict[str, float]`):\n",
    "                The values to log.\n",
    "            iterator (:obj:`tqdm`, `optional`):\n",
    "                A potential tqdm progress bar to write the logs on.\n",
    "        \"\"\"\n",
    "        # if hasattr(self, \"_log\"):\n",
    "        #     warnings.warn(\n",
    "        #         \"The `_log` method is deprecated and won't be called in a future version, define `log` in your subclass.\",\n",
    "        #         FutureWarning,\n",
    "        #     )\n",
    "        #     return self._log(logs, iterator=iterator)\n",
    "\n",
    "        if self.epoch is not None:\n",
    "            logs[\"epoch\"] = self.epoch\n",
    "        if self.global_step is None:\n",
    "            # when logging evaluation metrics without training\n",
    "            self.global_step = 0\n",
    "        if self.tb_writer:\n",
    "            for k, v in logs.items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    self.tb_writer.add_scalar(k, v, self.global_step)\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"Trainer is attempting to log a value of \"\n",
    "                        '\"%s\" of type %s for key \"%s\" as a scalar. '\n",
    "                        \"This invocation of Tensorboard's writer.add_scalar() \"\n",
    "                        \"is incorrect so we dropped this attribute.\",\n",
    "                        v,\n",
    "                        type(v),\n",
    "                        k,\n",
    "                    )\n",
    "            self.tb_writer.flush()\n",
    "        if is_wandb_available():\n",
    "            if self.is_world_master():\n",
    "                wandb.log(logs, step=self.global_step)\n",
    "        output = {**logs, **{\"step\": self.global_step}}\n",
    "        if iterator is not None:\n",
    "            iterator.write(output)\n",
    "        else:\n",
    "            logger.info(output)\n",
    "\n",
    "    def _intermediate_eval(self, datasets, epoch, output_eval_file, description, model_to_test_with):\n",
    "\n",
    "        \"\"\"\n",
    "        Helper function to call eval() method if and when you want to evaluate after say each epoch,\n",
    "        instead having to wait till the end of all epochs\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"*******End of training.Going to run evaluation on {description} \")\n",
    "\n",
    "        if \"dev\" in description:\n",
    "            self.compute_metrics = self.eval_compute_metrics\n",
    "        else:\n",
    "            if \"test\" in description:\n",
    "                self.compute_metrics = self.test_compute_metrics\n",
    "\n",
    "        assert self.compute_metrics is not None\n",
    "        # Evaluation\n",
    "        eval_results = {}\n",
    "        datasetss = [datasets]\n",
    "        for dataset in datasetss:\n",
    "            eval_result = None\n",
    "            if \"dev\" in description:\n",
    "                eval_result, plain_text, gold_labels, predictions = self.evaluate(model_to_test_with,\n",
    "                                                                                  eval_dataset=dataset)\n",
    "            else:\n",
    "                if \"test\" in description:\n",
    "                    eval_result, plain_text, gold_labels, predictions = self.evaluate_on_test_partition(\n",
    "                        model_to_test_with, test_dataset=dataset)\n",
    "            assert eval_result is not None\n",
    "\n",
    "            if self.is_world_master():\n",
    "                with open(output_eval_file, \"a\") as writer:\n",
    "                    for key, value in eval_result.items():\n",
    "                        logger.info(\"  %s = %s\", key, value)\n",
    "                        writer.write(\"%s = %s\\n\" % (key, value))\n",
    "            eval_results.update(eval_result)\n",
    "        return eval_result, plain_text, gold_labels, predictions\n",
    "\n",
    "    def _save(self, output_dir: Optional[str] = None):\n",
    "        output_dir = output_dir if output_dir is not None else self.args.output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "        # Save a trained model and configuration using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        assert self.model is not None\n",
    "        if not isinstance(self.model, PreTrainedModel):\n",
    "            raise ValueError(\"Trainer.model appears to not be a PreTrainedModel\")\n",
    "        self.model.save_pretrained(output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "\n",
    "    def save_model(self, output_dir: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Will save the model, so you can reload it using :obj:`from_pretrained()`.\n",
    "\n",
    "        Will only save from the world_master process (unless in TPUs).\n",
    "        \"\"\"\n",
    "\n",
    "        if is_torch_tpu_available():\n",
    "            self._save_tpu(output_dir)\n",
    "        elif self.is_world_master():\n",
    "            self._save(output_dir)\n",
    "\n",
    "    def prediction_step(\n",
    "            self, model: nn.Module, inputs: Dict[str, torch.Tensor,], prediction_loss_only: bool\n",
    "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on :obj:`model` using obj:`inputs`.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (:obj:`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "\n",
    "        Return:\n",
    "            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "            A tuple with the loss, logits and labels (each being optional).\n",
    "        \"\"\"\n",
    "        has_labels = any(inputs.get(k) is not None for k in [\"labels\", \"lm_labels\", \"masked_lm_labels\"])\n",
    "\n",
    "        inputs = self._prepare_inputs(inputs, model)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            if has_labels:\n",
    "                loss, logits = outputs[:2]\n",
    "                loss = loss.mean().item()\n",
    "            else:\n",
    "                loss = None\n",
    "                logits = outputs[0]\n",
    "            if self.args.past_index >= 0:\n",
    "                self._past = outputs[self.args.past_index if has_labels else self.args.past_index - 1]\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        labels = inputs.get(\"labels\")\n",
    "        if labels is not None:\n",
    "            labels = labels.detach()\n",
    "        return (loss, logits.detach(), labels)\n",
    "\n",
    "    def get_git_info(self):\n",
    "        repo = git.Repo(search_parent_directories=True)\n",
    "\n",
    "        repo_sha = str(repo.head.object.hexsha),\n",
    "        repo_short_sha = str(repo.git.rev_parse(repo_sha, short=6))\n",
    "\n",
    "        repo_infos = {\n",
    "            \"repo_id\": str(repo),\n",
    "            \"repo_sha\": str(repo.head.object.hexsha),\n",
    "            \"repo_branch\": str(repo.active_branch),\n",
    "            \"repo_short_sha\": repo_short_sha\n",
    "        }\n",
    "        return repo_infos\n",
    "\n",
    "    def train_1teacher_1student(self, model_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Main training entry point.\n",
    "        Args:\n",
    "            model_path:\n",
    "                (Optional) Local path to model if model to train has been instantiated from a local path\n",
    "                If present, we will try reloading the optimizer/scheduler states from there.\n",
    "        \"\"\"\n",
    "        train_dataloader = self.get_train_dataloader_two_parallel_datasets()\n",
    "        if self.args.max_steps > 0:\n",
    "            t_total = self.args.max_steps\n",
    "            num_train_epochs = (\n",
    "                    self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n",
    "            )\n",
    "        else:\n",
    "            t_total = int(len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs)\n",
    "            num_train_epochs = self.args.num_train_epochs\n",
    "\n",
    "        model_teacher = self.lex_teacher_model\n",
    "        model_student = self.delex_student_model\n",
    "\n",
    "        weight_consistency_loss = 1\n",
    "        weight_classification_loss = 0.0875\n",
    "\n",
    "        optimizer = None\n",
    "        scheduler = None\n",
    "\n",
    "        # these flags are used for testing purposes. IDeally when running in student teacher mode this should be\n",
    "        # flag_run_both=True. Other two flags are to test by loading each of these models independently from within\n",
    "        # the same trainer class\n",
    "        flag_run_teacher_alone = False\n",
    "        flag_run_student_alone = True\n",
    "        flag_run_both = False\n",
    "\n",
    "        if (flag_run_both):\n",
    "            optimizer, scheduler = self.get_optimizers_for_student_teacher(num_training_steps=self.args.lr_max_value)\n",
    "            assert optimizer is not None\n",
    "            assert scheduler is not None\n",
    "        else:\n",
    "            if (flag_run_teacher_alone):\n",
    "                optimizer, scheduler = self.get_optimizer(model_teacher, num_training_steps=self.args.lr_max_value)\n",
    "                assert optimizer is not None\n",
    "                assert scheduler is not None\n",
    "            else:\n",
    "                if (flag_run_student_alone):\n",
    "                    optimizer, scheduler = self.get_optimizer(model_student, num_training_steps=self.args.lr_max_value)\n",
    "                    assert optimizer is not None\n",
    "                    assert scheduler is not None\n",
    "\n",
    "        # Check if saved optimizer or scheduler states exist\n",
    "        if (\n",
    "                model_path is not None\n",
    "                and os.path.isfile(os.path.join(model_path, \"optimizer.pt\"))\n",
    "                and os.path.isfile(os.path.join(model_path, \"scheduler.pt\"))\n",
    "        ):\n",
    "            # Load in optimizer and scheduler states\n",
    "            optimizer.load_state_dict(\n",
    "                torch.load(os.path.join(model_path, \"optimizer.pt\"), map_location=self.args.device)\n",
    "            )\n",
    "            scheduler.load_state_dict(torch.load(os.path.join(model_path, \"scheduler.pt\")))\n",
    "\n",
    "        if self.args.fp16:\n",
    "            if not is_apex_available():\n",
    "                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "            model_teacher, optimizer = amp.initialize(model_teacher, optimizer, opt_level=self.args.fp16_opt_level)\n",
    "            model_student, optimizer = amp.initialize(model_student, optimizer, opt_level=self.args.fp16_opt_level)\n",
    "\n",
    "        # multi-gpu training (should be after apex fp16 initialization)\n",
    "        if self.args.n_gpu > 1:\n",
    "            logger.info(\n",
    "                f\"found that self.args.Nn_gpu >1. going to exit.\")\n",
    "            import sys\n",
    "            sys.exit()\n",
    "\n",
    "            model_teacher = torch.nn.DataParallel(model_teacher)\n",
    "            model_student = torch.nn.DataParallel(model_student)\n",
    "\n",
    "        # Distributed training (should be after apex fp16 initialization)\n",
    "        if self.args.local_rank != -1:\n",
    "            model_teacher = torch.nn.parallel.DistributedDataParallel(\n",
    "                model_teacher,\n",
    "                device_ids=[self.args.local_rank],\n",
    "                output_device=self.args.local_rank,\n",
    "                find_unused_parameters=True,\n",
    "            )\n",
    "        if self.args.local_rank != -1:\n",
    "            model_student = torch.nn.parallel.DistributedDataParallel(\n",
    "                model_student,\n",
    "                device_ids=[self.args.local_rank],\n",
    "                output_device=self.args.local_rank,\n",
    "                find_unused_parameters=True,\n",
    "            )\n",
    "\n",
    "        if self.tb_writer is not None:\n",
    "            self.tb_writer.add_text(\"args\", self.args.to_json_string())\n",
    "            self.tb_writer.add_hparams(self.args.to_sanitized_dict(), metric_dict={})\n",
    "\n",
    "        # Train!\n",
    "        if is_torch_tpu_available():\n",
    "            total_train_batch_size = self.args.train_batch_size * xm.xrt_world_size()\n",
    "        else:\n",
    "            total_train_batch_size = (\n",
    "                    self.args.train_batch_size\n",
    "                    * self.args.gradient_accumulation_steps\n",
    "                    * (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1)\n",
    "            )\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", self.num_examples(train_dataloader))\n",
    "        logger.info(\"  Num Epochs = %d\", num_train_epochs)\n",
    "        logger.info(\"  Instantaneous batch size per device = %d\", self.args.per_device_train_batch_size)\n",
    "        logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\", total_train_batch_size)\n",
    "        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n",
    "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.epoch = 0\n",
    "        epochs_trained = 0\n",
    "        steps_trained_in_current_epoch = 0\n",
    "        # Check if continuing training from a checkpoint\n",
    "        if model_path is not None:\n",
    "            # set global_step to global_step of last saved checkpoint from model path\n",
    "            try:\n",
    "                self.global_step = int(model_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "                epochs_trained = self.global_step // (len(train_dataloader) // self.args.gradient_accumulation_steps)\n",
    "                steps_trained_in_current_epoch = self.global_step % (\n",
    "                        len(train_dataloader) // self.args.gradient_accumulation_steps\n",
    "                )\n",
    "\n",
    "                logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "                logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "                logger.info(\"  Continuing training from global step %d\", self.global_step)\n",
    "                logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "            except ValueError:\n",
    "                self.global_step = 0\n",
    "                logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "        tr_loss_lex_float = 0.0\n",
    "        tr_loss_delex_float = 0.0\n",
    "        logging_loss = 0.0\n",
    "        model_teacher.zero_grad()\n",
    "        model_student.zero_grad()\n",
    "\n",
    "        train_iterator = trange(\n",
    "            epochs_trained, int(num_train_epochs), desc=\"Epoch\", disable=not self.is_local_master()\n",
    "        )\n",
    "\n",
    "        git_details = self.get_git_info()\n",
    "\n",
    "        # empty out the file which stores intermediate evaluations\n",
    "        output_dir_absolute_path = os.path.join(os.getcwd(), self.args.output_dir)\n",
    "        dev_partition_evaluation_output_file_path = output_dir_absolute_path + \"intermediate_evaluation_on_dev_partition_results_\" + \\\n",
    "                                                    git_details['repo_short_sha'] + \".txt\"\n",
    "        # empty out the\n",
    "        with open(dev_partition_evaluation_output_file_path, \"w\") as writer:\n",
    "            writer.write(\"\")\n",
    "\n",
    "        test_partition_evaluation_output_file_path = output_dir_absolute_path + \"intermediate_evaluation_on_test_partition_results_\" + \\\n",
    "                                                     git_details['repo_short_sha'] + \".txt\"\n",
    "        # empty out the\n",
    "        with open(test_partition_evaluation_output_file_path, \"w\") as writer:\n",
    "            writer.write(\"\")\n",
    "\n",
    "        # empty out the file which stores intermediate evaluations\n",
    "        predictions_on_test_file_path = output_dir_absolute_path + \"predictions_on_test_partition_\" + git_details[\n",
    "            'repo_short_sha'] + \".txt\"\n",
    "        with open(predictions_on_test_file_path, \"w\") as writer:\n",
    "            writer.write(\"\")\n",
    "\n",
    "        best_fnc_score = 0\n",
    "        best_acc = 0\n",
    "\n",
    "        # for each epoch\n",
    "\n",
    "        for epoch in train_iterator:\n",
    "            logger.debug(\"just got inside for epoch in train_iterator\")\n",
    "\n",
    "            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n",
    "                train_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "            if is_torch_tpu_available():\n",
    "                logger.debug(\"found that is_torch_tpu_available is true\")\n",
    "\n",
    "                parallel_loader = pl.ParallelLoader(train_dataloader, [self.args.device]).per_device_loader(\n",
    "                    self.args.device\n",
    "                )\n",
    "                epoch_iterator = tqdm(parallel_loader, desc=\"batches\", disable=not self.is_local_master())\n",
    "            else:\n",
    "                logger.debug(\"found that is_torch_tpu_available is false\")\n",
    "                epoch_iterator = tqdm(train_dataloader, desc=\"batches\", disable=not self.is_local_master())\n",
    "\n",
    "            # for each batch\n",
    "            for step, (input_lex, input_delex) in enumerate(epoch_iterator):\n",
    "                logger.debug(\"just got inside for step in enumerate epoch_iterator. i.e for each batch\")\n",
    "\n",
    "                # Skip past any already trained steps if resuming training\n",
    "                if steps_trained_in_current_epoch > 0:\n",
    "                    steps_trained_in_current_epoch -= 1\n",
    "                    continue\n",
    "                assert input_lex['labels'].tolist() == input_delex['labels'].tolist()\n",
    "\n",
    "                # model returns # (loss), logits, (hidden_states), (attentions)\n",
    "                tr_loss_lex, outputs_lex = self.get_classification_loss(model_teacher, input_lex, optimizer)\n",
    "                tr_loss_delex, outputs_delex = self.get_classification_loss(model_student, input_delex, optimizer)\n",
    "\n",
    "                if (flag_run_both):\n",
    "                    combined_classification_loss = tr_loss_lex + tr_loss_delex\n",
    "\n",
    "                else:\n",
    "                    if (flag_run_teacher_alone):\n",
    "                        combined_classification_loss = tr_loss_lex\n",
    "                    else:\n",
    "                        if (flag_run_student_alone):\n",
    "                            combined_classification_loss = tr_loss_delex\n",
    "\n",
    "                logger.debug(\"finished getting classification loss\")\n",
    "\n",
    "                # outputs contains in that order # (loss), logits, (hidden_states), (attentions)-src/transformers/modeling_bert.py\n",
    "                logits_lex = outputs_lex[1]\n",
    "                logits_delex = outputs_delex[1]\n",
    "                consistency_loss = self.get_consistency_loss(logits_lex, logits_delex, \"mse\")\n",
    "\n",
    "                if (flag_run_both):\n",
    "                    combined_loss = (weight_classification_loss * combined_classification_loss) + (\n",
    "                                weight_consistency_loss * consistency_loss)\n",
    "                else:\n",
    "                    combined_loss = combined_classification_loss\n",
    "\n",
    "                if self.args.fp16:\n",
    "                    logger.info(\"self.args.fp16 is true\")\n",
    "                    with amp.scale_loss(combined_loss, optimizer) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "                    with amp.scale_loss(combined_loss, optimizer) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "                else:\n",
    "                    logger.debug(\"self.args.fp16 is false\")\n",
    "                    combined_loss.backward()\n",
    "\n",
    "                    logger.debug(\"just got done with combined_loss.backward()\")\n",
    "\n",
    "                tr_loss_lex_float += tr_loss_lex.item()\n",
    "                tr_loss_delex_float += tr_loss_delex.item()\n",
    "\n",
    "                if (step + 1) % self.args.gradient_accumulation_steps == 0 or (\n",
    "                        # last step in epoch but step is always smaller than gradient_accumulation_steps\n",
    "                        len(epoch_iterator) <= self.args.gradient_accumulation_steps\n",
    "                        and (step + 1) == len(epoch_iterator)\n",
    "                ):\n",
    "                    logger.debug(\"got inside if condition for if(step+1. i.e last step in epoch)\")\n",
    "\n",
    "                    if self.args.fp16:\n",
    "                        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), self.args.max_grad_norm)\n",
    "                    else:\n",
    "                        if (flag_run_both):\n",
    "                            torch.nn.utils.clip_grad_norm_(model_student.parameters(), self.args.max_grad_norm)\n",
    "                            torch.nn.utils.clip_grad_norm_(model_teacher.parameters(), self.args.max_grad_norm)\n",
    "\n",
    "                        else:\n",
    "                            if (flag_run_teacher_alone):\n",
    "                                torch.nn.utils.clip_grad_norm_(model_teacher.parameters(), self.args.max_grad_norm)\n",
    "                            else:\n",
    "                                if (flag_run_student_alone):\n",
    "                                    torch.nn.utils.clip_grad_norm_(model_student.parameters(), self.args.max_grad_norm)\n",
    "\n",
    "                    logger.debug(\"just done with grad clipping)\")\n",
    "\n",
    "                    if is_torch_tpu_available():\n",
    "                        xm.optimizer_step(optimizer)\n",
    "                    else:\n",
    "                        optimizer.step()\n",
    "                        logger.debug(\"just done withn optimixer.step)\")\n",
    "                    scheduler.step()\n",
    "                    if (flag_run_both):\n",
    "                        model_teacher.zero_grad()\n",
    "                        model_student.zero_grad()\n",
    "\n",
    "                    else:\n",
    "                        if (flag_run_teacher_alone):\n",
    "                            model_teacher.zero_grad()\n",
    "                        else:\n",
    "                            if (flag_run_student_alone):\n",
    "                                model_student.zero_grad()\n",
    "\n",
    "                    self.global_step += 1\n",
    "                    self.epoch = epoch + (step + 1) / len(epoch_iterator)\n",
    "\n",
    "                    if (self.args.logging_steps > 0 and self.global_step % self.args.logging_steps == 0) or (\n",
    "                            self.global_step == 1 and self.args.logging_first_step\n",
    "                    ):\n",
    "                        logs: Dict[str, float] = {}\n",
    "                        logs[\"loss\"] = (tr_loss_lex_float - logging_loss) / self.args.logging_steps\n",
    "                        # backward compatibility for pytorch schedulers\n",
    "                        logs[\"learning_rate\"] = (\n",
    "                            scheduler.get_last_lr()[0]\n",
    "                            if version.parse(torch.__version__) >= version.parse(\"1.4\")\n",
    "                            else scheduler.get_lr()[0]\n",
    "                        )\n",
    "                        logging_loss = tr_loss_lex_float\n",
    "\n",
    "                        self.log(logs)\n",
    "\n",
    "                        # if self.args.evaluate_during_training:\n",
    "                        #     self.evaluate()\n",
    "\n",
    "                    if self.args.save_steps > 0 and self.global_step % self.args.save_steps == 0:\n",
    "                        # In all cases (even distributed/parallel), self.model is always a reference\n",
    "                        # to the model we want to save.\n",
    "                        # update: in student teacher setting since there are way too many model words going on, we will ezxplicitly pass the model to save\n",
    "\n",
    "                        if (flag_run_both):\n",
    "                            if hasattr(model_teacher, \"module\"):\n",
    "                                assert model_teacher.module is self.lex_teacher_model.module\n",
    "                                assert model_student.module is self.delex_student_model.module\n",
    "\n",
    "                            else:\n",
    "                                assert model_teacher is self.lex_teacher_model\n",
    "                                assert model_student is self.delex_student_model\n",
    "\n",
    "                            self.model = model_teacher\n",
    "                            output_dir = os.path.join(self.args.output_dir,\n",
    "                                                      f\"model_teacher_{PREFIX_CHECKPOINT_DIR}-{self.global_step}\")\n",
    "                            assert self.model is not None\n",
    "                            self.save_model(output_dir)\n",
    "\n",
    "                            self.model = model_student\n",
    "                            output_dir = os.path.join(self.args.output_dir,\n",
    "                                                      f\"model_student_{PREFIX_CHECKPOINT_DIR}-{self.global_step}\")\n",
    "                            assert self.model is not None\n",
    "                            self.save_model(output_dir)\n",
    "\n",
    "\n",
    "                        else:\n",
    "                            if (flag_run_teacher_alone):\n",
    "                                if hasattr(model_teacher, \"module\"):\n",
    "                                    assert model_teacher.module is self.lex_teacher_model.module\n",
    "                                else:\n",
    "                                    assert model_teacher is self.lex_teacher_model\n",
    "                                self.model = model_teacher\n",
    "                                output_dir = os.path.join(self.args.output_dir,\n",
    "                                                          f\"model_teacher_{PREFIX_CHECKPOINT_DIR}-{self.global_step}\")\n",
    "                                self.save_model(output_dir)\n",
    "                            else:\n",
    "                                if (flag_run_student_alone):\n",
    "                                    if hasattr(model_teacher, \"module\"):\n",
    "                                        assert model_student.module is self.delex_student_model.module\n",
    "                                    else:\n",
    "                                        assert model_student is self.delex_student_model\n",
    "                                self.model = model_student\n",
    "                                output_dir = os.path.join(self.args.output_dir,\n",
    "                                                          f\"model_student_{PREFIX_CHECKPOINT_DIR}-{self.global_step}\")\n",
    "                                self.save_model(output_dir)\n",
    "\n",
    "                        # Save model checkpoint\n",
    "\n",
    "                        if hasattr(model_student, \"module\"):\n",
    "                            assert model_student.module is self.delex_student_model\n",
    "                        else:\n",
    "                            assert model_student is self.delex_student_model\n",
    "                        # Save model checkpoint\n",
    "\n",
    "                        if self.is_world_master():\n",
    "                            self._rotate_checkpoints()\n",
    "\n",
    "                        if is_torch_tpu_available():\n",
    "                            xm.rendezvous(\"saving_optimizer_states\")\n",
    "                            xm.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                            xm.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                        elif self.is_world_master():\n",
    "                            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                            torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "\n",
    "                if self.args.max_steps > 0 and self.global_step > self.args.max_steps:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "\n",
    "            trained_model = None\n",
    "            if (flag_run_both):\n",
    "                trained_model = model_student\n",
    "            else:\n",
    "                if (flag_run_teacher_alone):\n",
    "                    trained_model = model_teacher\n",
    "                else:\n",
    "                    if (flag_run_student_alone):\n",
    "                        trained_model = model_student\n",
    "\n",
    "            assert trained_model is not None\n",
    "\n",
    "            dev_partition_evaluation_result, plain_text, gold_labels, predictions = self._intermediate_eval(\n",
    "                datasets=self.eval_dataset,\n",
    "                epoch=epoch,\n",
    "                output_eval_file=dev_partition_evaluation_output_file_path,\n",
    "                description=\"dev_partition\", model_to_test_with=trained_model)\n",
    "\n",
    "            test_partition_evaluation_result, plain_text, gold_labels, predictions_logits = self._intermediate_eval(\n",
    "                datasets=self.test_dataset,\n",
    "                epoch=epoch, output_eval_file=test_partition_evaluation_output_file_path, description=\"test_partition\",\n",
    "                model_to_test_with=trained_model)\n",
    "\n",
    "            fnc_score_test_partition = test_partition_evaluation_result['eval_acc']['cross_domain_fnc_score']\n",
    "            accuracy_test_partition = test_partition_evaluation_result['eval_acc']['cross_domain_acc']\n",
    "\n",
    "            if fnc_score_test_partition > best_fnc_score:\n",
    "                best_fnc_score = fnc_score_test_partition\n",
    "\n",
    "                logger.info(f\"found that the current fncscore:{fnc_score_test_partition} in epoch \"\n",
    "                            f\"{epoch} beats the bestfncscore so far i.e ={best_fnc_score}. going to prediction\"\n",
    "                            f\"on test partition and save that and model to disk\")\n",
    "                # if the accuracy or fnc_score_test_partition beats the highest so far, write predictions to disk\n",
    "\n",
    "                self.write_predictions_to_disk(plain_text, gold_labels, predictions_logits,\n",
    "                                               predictions_on_test_file_path,\n",
    "                                               self.test_dataset)\n",
    "\n",
    "                # Save model checkpoint\n",
    "                self.model = trained_model\n",
    "                output_dir = os.path.join(self.args.output_dir)\n",
    "                self.save_model(output_dir)\n",
    "\n",
    "                # if self.is_world_master():\n",
    "                #     self._rotate_checkpoints()\n",
    "\n",
    "                if is_torch_tpu_available():\n",
    "                    xm.rendezvous(\"saving_optimizer_states\")\n",
    "                    xm.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    xm.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                elif self.is_world_master():\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "\n",
    "            if accuracy_test_partition > best_acc:\n",
    "                best_acc = accuracy_test_partition\n",
    "\n",
    "            logger.info(\n",
    "                f\"********************************end of epoch {epoch+1}************************************************************************\")\n",
    "            if self.args.max_steps > 0 and self.global_step > self.args.max_steps:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "            if self.args.tpu_metrics_debug:\n",
    "                # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "                xm.master_print(met.metrics_report())\n",
    "\n",
    "        if self.tb_writer:\n",
    "            self.tb_writer.close()\n",
    "\n",
    "        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n",
    "\n",
    "        # Note: returning for testing purposes only. All performance evaluation measures by now are written to disk.\n",
    "        # Note that the assumption here is that the test will be run for 1 epoch only. ELse have to return the best dev and test partition scores\n",
    "        return dev_partition_evaluation_result, test_partition_evaluation_result\n",
    "\n",
    "    def log(self, logs: Dict[str, float], iterator: Optional[tqdm] = None) -> None:\n",
    "        \"\"\"\n",
    "        Log :obj:`logs` on the various objects watching training.\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            logs (:obj:`Dict[str, float]`):\n",
    "                The values to log.\n",
    "            iterator (:obj:`tqdm`, `optional`):\n",
    "                A potential tqdm progress bar to write the logs on.\n",
    "        \"\"\"\n",
    "        # if hasattr(self, \"_log\"):\n",
    "        #     warnings.warn(\n",
    "        #         \"The `_log` method is deprecated and won't be called in a future version, define `log` in your subclass.\",\n",
    "        #         FutureWarning,\n",
    "        #     )\n",
    "        #     return self._log(logs, iterator=iterator)\n",
    "\n",
    "        if self.epoch is not None:\n",
    "            logs[\"epoch\"] = self.epoch\n",
    "        if self.global_step is None:\n",
    "            # when logging evaluation metrics without training\n",
    "            self.global_step = 0\n",
    "        log_for_wandb = {}\n",
    "        if self.tb_writer:\n",
    "            for k, v in logs.items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    self.tb_writer.add_scalar(k, v, self.global_step)\n",
    "                    log_for_wandb[k] = v\n",
    "                else:\n",
    "                    if isinstance(v, (dict)):\n",
    "                        for k2, v2 in v.items():\n",
    "                            if isinstance(v2, (int, float)):\n",
    "                                self.tb_writer.add_scalar(k2, v2, self.global_step)\n",
    "                                log_for_wandb[k2] = v2\n",
    "                            else:\n",
    "                                logger.warning(\n",
    "                                    f\"Trainer is attempting to log a valuefor key {k2}as a scalar.\"\n",
    "                                    f\"This invocation of Tensorboard's writer.add_scalar()\"\n",
    "                                    f\" is incorrect so we dropped this attribute.\",\n",
    "                                )\n",
    "                                logger.debug(v2)\n",
    "            self.tb_writer.flush()\n",
    "        if is_wandb_available():\n",
    "            if self.is_world_master():\n",
    "                if len(log_for_wandb.items()) > 0:\n",
    "                    wandb.log(log_for_wandb, step=int(self.epoch))\n",
    "        output = {**logs, **{\"step\": self.global_step}}\n",
    "        if iterator is not None:\n",
    "            iterator.write(output)\n",
    "        else:\n",
    "            logger.debug(output)\n",
    "\n",
    "    def _training_step(\n",
    "            self, model: nn.Module, inputs: Dict[str, torch.Tensor], optimizer: torch.optim.Optimizer\n",
    "    ) -> float:\n",
    "        model.train()\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(self.args.device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            logger.info(\n",
    "                f\"found that self.args.Nn_gpu >1. going to exit.\")\n",
    "            import sys\n",
    "            sys.exit()\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.args.fp16:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def is_local_master(self) -> bool:\n",
    "        if is_torch_tpu_available():\n",
    "            return xm.is_master_ordinal(local=True)\n",
    "        else:\n",
    "            return self.args.local_rank in [-1, 0]\n",
    "\n",
    "    def is_world_master(self) -> bool:\n",
    "        \"\"\"\n",
    "        This will be True only in one process, even in distributed mode,\n",
    "        even when training on multiple machines.\n",
    "        \"\"\"\n",
    "        if is_torch_tpu_available():\n",
    "            return xm.is_master_ordinal(local=False)\n",
    "        else:\n",
    "            return self.args.local_rank == -1 or torch.distributed.get_rank() == 0\n",
    "\n",
    "    def save_model(self, output_dir: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Will save the model, so you can reload it using :obj:`from_pretrained()`.\n",
    "\n",
    "        Will only save from the world_master process (unless in TPUs).\n",
    "        \"\"\"\n",
    "\n",
    "        if is_torch_tpu_available():\n",
    "            self._save_tpu(output_dir)\n",
    "        elif self.is_world_master():\n",
    "            self._save(output_dir)\n",
    "\n",
    "    def _save_tpu(self, output_dir: Optional[str] = None):\n",
    "        output_dir = output_dir if output_dir is not None else self.args.output_dir\n",
    "        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "        if xm.is_master_ordinal():\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Save a trained model and configuration using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        if not isinstance(self.model, PreTrainedModel):\n",
    "            raise ValueError(\"Trainer.model appears to not be a PreTrainedModel\")\n",
    "\n",
    "        xm.rendezvous(\"saving_checkpoint\")\n",
    "        self.model.save_pretrained(output_dir)\n",
    "\n",
    "    def _save(self, output_dir: Optional[str] = None):\n",
    "        output_dir = output_dir if output_dir is not None else self.args.output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "        # Save a trained model and configuration using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        if not isinstance(self.model, PreTrainedModel):\n",
    "            raise ValueError(\"Trainer.model appears to not be a PreTrainedModel\")\n",
    "        self.model.save_pretrained(output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "\n",
    "    def _sorted_checkpoints(self, checkpoint_prefix=PREFIX_CHECKPOINT_DIR, use_mtime=False) -> List[str]:\n",
    "        ordering_and_checkpoint_path = []\n",
    "\n",
    "        glob_checkpoints = [str(x) for x in Path(self.args.output_dir).glob(f\"{checkpoint_prefix}-*\")]\n",
    "\n",
    "        for path in glob_checkpoints:\n",
    "            if use_mtime:\n",
    "                ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "            else:\n",
    "                regex_match = re.match(f\".*{checkpoint_prefix}-([0-9]+)\", path)\n",
    "                if regex_match and regex_match.groups():\n",
    "                    ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "        checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "        checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "        return checkpoints_sorted\n",
    "\n",
    "    def _rotate_checkpoints(self, use_mtime=False) -> None:\n",
    "        if self.args.save_total_limit is None or self.args.save_total_limit <= 0:\n",
    "            return\n",
    "\n",
    "        # Check if we should delete older checkpoint(s)\n",
    "        checkpoints_sorted = self._sorted_checkpoints(use_mtime=use_mtime)\n",
    "        if len(checkpoints_sorted) <= self.args.save_total_limit:\n",
    "            return\n",
    "\n",
    "        number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - self.args.save_total_limit)\n",
    "        checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "        for checkpoint in checkpoints_to_be_deleted:\n",
    "            logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "            shutil.rmtree(checkpoint)\n",
    "\n",
    "    def get_classification_loss(\n",
    "            self, model: nn.Module, inputs: Dict[str, torch.Tensor], optimizer: torch.optim.Optimizer\n",
    "    ) -> float:\n",
    "        '''\n",
    "        similar to _training_step however returns loss instead of doing .backward\n",
    "        Args:\n",
    "            model:\n",
    "            inputs:\n",
    "            optimizer:\n",
    "        Returns:loss\n",
    "        '''\n",
    "        model.train()\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(self.args.device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            logger.info(\n",
    "                f\"found that self.args.Nn_gpu >1. going to exit.\")\n",
    "            import sys\n",
    "            sys.exit()\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        return loss, outputs\n",
    "\n",
    "    def get_logits(\n",
    "            self, model: nn.Module, inputs: Dict[str, torch.Tensor], optimizer: torch.optim.Optimizer\n",
    "    ):\n",
    "        '''\n",
    "        similar to _training_step however returns loss instead of doing .backward\n",
    "        Args:\n",
    "            model:\n",
    "            inputs:\n",
    "            optimizer:\n",
    "        Returns:loss\n",
    "        '''\n",
    "        model.train()\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(self.args.device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        return outputs\n",
    "\n",
    "    def get_consistency_loss(\n",
    "            self, logit1, logit2, loss_function):\n",
    "        '''\n",
    "        similar to _training_step however returns loss instead of doing .backward\n",
    "        Args:\n",
    "\n",
    "            model:\n",
    "            inputs:\n",
    "            optimizer:\n",
    "        Returns:loss\n",
    "        '''\n",
    "\n",
    "        if (loss_function == \"mse\"):\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logit1.view(-1), logit2.view(-1))\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            logger.info(\n",
    "                f\"found that self.args.Nn_gpu >1. going to exit.\")\n",
    "            import sys\n",
    "            sys.exit()\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def is_local_master(self) -> bool:\n",
    "        if is_torch_tpu_available():\n",
    "            return xm.is_master_ordinal(local=True)\n",
    "        else:\n",
    "            return self.args.local_rank in [-1, 0]\n",
    "\n",
    "    def is_world_master(self) -> bool:\n",
    "        \"\"\"\n",
    "        This will be True only in one process, even in distributed mode,\n",
    "        even when training on multiple machines.\n",
    "        \"\"\"\n",
    "        if is_torch_tpu_available():\n",
    "            return xm.is_master_ordinal(local=False)\n",
    "        else:\n",
    "            return self.args.local_rank == -1 or torch.distributed.get_rank() == 0\n",
    "\n",
    "    def prediction_loop(\n",
    "            self, dataloader: DataLoader, model_to_test_with, description: str,\n",
    "            prediction_loss_only: Optional[bool] = None\n",
    "    ) -> PredictionOutput:\n",
    "        \"\"\"\n",
    "        Prediction/evaluation loop, shared by :obj:`Trainer.evaluate()` and :obj:`Trainer.predict()`.\n",
    "\n",
    "        Works both with or without labels.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"_prediction_loop\"):\n",
    "            warnings.warn(\n",
    "                \"The `_prediction_loop` method is deprecated and won't be called in a future version, define `prediction_loop` in your subclass.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            return self._prediction_loop(dataloader, description, prediction_loss_only=prediction_loss_only)\n",
    "\n",
    "        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else self.prediction_loss_only\n",
    "\n",
    "        model = model_to_test_with\n",
    "        # multi-gpu eval\n",
    "        if self.args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            logger.info(\n",
    "                f\"found that self.args.Nn_gpu >1. going to exit.\")\n",
    "            import sys\n",
    "            sys.exit()\n",
    "\n",
    "        # Note: in torch.distributed mode, there's no point in wrapping the model\n",
    "        # inside a DistributedDataParallel as we'll be under `no_grad` anyways.\n",
    "\n",
    "        batch_size = dataloader.batch_size\n",
    "        logger.debug(\"***** Running %s at epoch number:%s *****\", description, self.epoch)\n",
    "        logger.debug(\"  Num examples = %d\", self.num_examples(dataloader))\n",
    "        logger.debug(\"  Batch size = %d\", batch_size)\n",
    "        eval_losses: List[float] = []\n",
    "        preds: torch.Tensor = None\n",
    "        label_ids: torch.Tensor = None\n",
    "        model.eval()\n",
    "\n",
    "        if is_torch_tpu_available():\n",
    "            dataloader = pl.ParallelLoader(dataloader, [self.args.device]).per_device_loader(self.args.device)\n",
    "\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = None\n",
    "        plain_text_full = []\n",
    "        for inputs in tqdm(dataloader, desc=description):\n",
    "            plain_text_batch = self.delex_tokenizer.batch_decode(inputs['input_ids'])\n",
    "            plain_text_full.extend(plain_text_batch)\n",
    "            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only)\n",
    "            if loss is not None:\n",
    "                eval_losses.append(loss)\n",
    "            if logits is not None:\n",
    "                preds = logits if preds is None else torch.cat((preds, logits), dim=0)\n",
    "            if labels is not None:\n",
    "                label_ids = labels if label_ids is None else torch.cat((label_ids, labels), dim=0)\n",
    "\n",
    "        if self.args.past_index and hasattr(self, \"_past\"):\n",
    "            # Clean the state at the end of the evaluation loop\n",
    "            delattr(self, \"_past\")\n",
    "        logger.debug(f\" value of local rank is {self.args.local_rank}\")\n",
    "        if self.args.local_rank != -1:\n",
    "            logger.info(f\"found that local_rank is not minus one. value of local rank is {self.args.local_rank}\")\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "            # In distributed mode, concatenate all results from all nodes:\n",
    "            if preds is not None:\n",
    "                preds = self.distributed_concat(preds, num_total_examples=self.num_examples(dataloader))\n",
    "            if label_ids is not None:\n",
    "                label_ids = self.distributed_concat(label_ids, num_total_examples=self.num_examples(dataloader))\n",
    "        elif is_torch_tpu_available():\n",
    "            # tpu-comment: Get all predictions and labels from all worker shards of eval dataset\n",
    "            if preds is not None:\n",
    "                preds = xm.mesh_reduce(\"eval_preds\", preds, torch.cat)\n",
    "            if label_ids is not None:\n",
    "                label_ids = xm.mesh_reduce(\"eval_label_ids\", label_ids, torch.cat)\n",
    "\n",
    "        # Finally, turn the aggregated tensors into numpy arrays.\n",
    "        if preds is not None:\n",
    "            preds = preds.cpu().numpy()\n",
    "        if label_ids is not None:\n",
    "            label_ids = label_ids.cpu().numpy()\n",
    "\n",
    "        if self.compute_metrics is not None and preds is not None and label_ids is not None:\n",
    "            metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\n",
    "        else:\n",
    "            if self.compute_metrics is None:\n",
    "                logger.error(\"compute_metrics  is none. going to exit\")\n",
    "            if preds is None:\n",
    "                logger.error(\"preds  is none. going to exit\")\n",
    "            if label_ids is None:\n",
    "                logger.error(\"label_ids  is none. going to exit\")\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "            metrics = {}\n",
    "        if len(eval_losses) > 0:\n",
    "            metrics[\"eval_loss\"] = np.mean(eval_losses)\n",
    "\n",
    "        # Prefix all keys with eval_\n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(\"eval_\"):\n",
    "                metrics[f\"eval_{key}\"] = metrics.pop(key)\n",
    "        assert plain_text_full is not None\n",
    "        assert len(plain_text_full) == len(dataloader.dataset.features)\n",
    "\n",
    "        return PredictionOutput(predictions=preds, label_ids=label_ids, metrics=metrics), plain_text_full\n",
    "\n",
    "\n",
    "def call_html():\n",
    "  import IPython\n",
    "  from IPython.core.display import display\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
    "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))\n",
    "\n",
    "\n",
    "def get_git_info():\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "\n",
    "    repo_sha=str(repo.head.object.hexsha),\n",
    "    repo_short_sha= str(repo.git.rev_parse(repo_sha, short=6))\n",
    "\n",
    "    repo_infos = {\n",
    "        \"repo_id\": str(repo),\n",
    "        \"repo_sha\": str(repo.head.object.hexsha),\n",
    "        \"repo_branch\": str(repo.active_branch),\n",
    "        \"repo_short_sha\" :repo_short_sha\n",
    "    }\n",
    "    return repo_infos\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "def read_and_merge_config_entries():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(CONFIG_FILE_TO_TEST_WITH)\n",
    "    #assert not len(config.sections())==0\n",
    "    combined_configs=[]\n",
    "    for each_section in config.sections():\n",
    "        for (each_key, each_val) in config.items(each_section):\n",
    "            #some config entries of type bool just need to exist. doesnt need x=True.\n",
    "            # so now have to strip True out, until we findc a way to be able to pass it as bool itself\n",
    "            # so if True is a value, append only key\n",
    "            if (each_val==\"True\"):\n",
    "                combined_configs.append(\"--\"+each_key)\n",
    "            else:\n",
    "                combined_configs.append(\"--\" + each_key)\n",
    "                combined_configs.append(str(each_val).replace(\"\\\"\",\"\"))\n",
    "    combined_configs_str=\" \".join(combined_configs)\n",
    "    return combined_configs_str\n",
    "\n",
    "\n",
    "  \n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()\n",
    "\n",
    "\n",
    "    \n",
    "def run_loading_and_testing(model_args, data_args, training_args):\n",
    "    # Setup logging\n",
    "    git_details=get_git_info()\n",
    "\n",
    "    log_file_name=git_details['repo_short_sha']+\"_\"+(training_args.task_type)+\"_\"+(training_args.subtask_type)+\"_\"+str(model_args.model_name_or_path).replace(\"-\",\"_\")+\"_\"+data_args.task_name+\".log\"\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "        filename=log_file_name,\n",
    "        filemode='w'\n",
    "    )\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    try:\n",
    "\n",
    "        num_labels = glue_tasks_num_labels[data_args.task_name]\n",
    "        output_mode = glue_output_modes[data_args.task_name]\n",
    "\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Task not found: %s\" % (data_args.task_name))\n",
    "\n",
    "    # Load pretrained model_teacher and tokenizer_lex\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model_teacher & vocab.\n",
    "  \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        finetuning_task=data_args.task_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    tokenizer_lex = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        force_download=True,\n",
    "    )\n",
    "\n",
    "    #when in student-teacher mode, you need two tokenizers, one for lexicalized data, and one for the delexicalized data\n",
    "    # the regular tokenizer_lex will be used for lexicalized data and special one for delexicalized\n",
    "    tokenizer_delex = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        force_download=True,\n",
    "        tokenizer_type=\"delex\"\n",
    "    )\n",
    "\n",
    "\n",
    "    #this is needed for visualization\n",
    "    config.output_attentions=True\n",
    "\n",
    "    # Get datasets\n",
    "    if (training_args.do_train_1student_1teacher == True):\n",
    "        model_teacher = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "        model_student = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "\n",
    "    if (training_args.do_train_1student_1teacher == True):\n",
    "        # the task type must be delex, . also make sure the corresponding data has been downloaded in get_fever_fnc_data.sh\n",
    "        eval_dataset = (\n",
    "            GlueDataset(args=data_args, tokenizer=tokenizer_delex, task_type=\"delex\", mode=\"dev\",\n",
    "                        cache_dir=model_args.cache_dir)\n",
    "            if training_args.do_eval\n",
    "            else None\n",
    "        )\n",
    "    else:\n",
    "        if (training_args.task_type == \"mod1\"):\n",
    "            eval_dataset = (\n",
    "                GlueDataset(args=data_args, tokenizer=tokenizer_lex, task_type=\"mod1\", mode=\"dev\",\n",
    "                            cache_dir=model_args.cache_dir)\n",
    "                if training_args.do_eval\n",
    "                else None\n",
    "            )\n",
    "        else:\n",
    "            if (training_args.task_type == \"mod2\"):\n",
    "                eval_dataset = (\n",
    "                    GlueDataset(args=data_args, tokenizer=tokenizer_delex, task_type=\"mod2\", mode=\"dev\",\n",
    "                                cache_dir=model_args.cache_dir)\n",
    "                    if training_args.do_eval\n",
    "                    else None\n",
    "                )\n",
    "\n",
    "    if (training_args.do_train_1student_1teacher == True):\n",
    "        test_dataset = (\n",
    "            GlueDataset(data_args, tokenizer=tokenizer_delex, task_type=\"mod2\", mode=\"test\",\n",
    "                        cache_dir=model_args.cache_dir)\n",
    "            if training_args.do_predict\n",
    "            else None\n",
    "        )\n",
    "    else:\n",
    "        if (training_args.task_type == \"mod1\"):\n",
    "            test_dataset = (\n",
    "                GlueDataset(data_args, tokenizer=tokenizer_lex, task_type=\"mod1\", mode=\"test\",\n",
    "                            cache_dir=model_args.cache_dir)\n",
    "                if training_args.do_predict\n",
    "                else None\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            if (training_args.task_type == \"mod2\"):\n",
    "                test_dataset = (\n",
    "                    GlueDataset(data_args, tokenizer=tokenizer_delex, task_type=\"mod2\", mode=\"test\",\n",
    "                                cache_dir=model_args.cache_dir)\n",
    "                    if training_args.do_predict\n",
    "                    else None\n",
    "                )\n",
    "\n",
    "    def visualize(tokenizer,model):\n",
    "        ###code for visualization from  https://github.com/jessevig/bertviz\n",
    "        sentence_a = \"The cat sat on the mat\"\n",
    "        sentence_b = \"The dog lay on the rug\"\n",
    "        inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "        input_id_list = input_ids[0].tolist()  # Batch index 0\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "        call_html()\n",
    "        head_view(attention, tokens)\n",
    "        \n",
    "    def build_compute_metrics_fn(task_name: str) -> Callable[[EvalPrediction], Dict]:\n",
    "        def compute_metrics_fn(p: EvalPrediction):\n",
    "            if output_mode == \"classification\":\n",
    "                preds = np.argmax(p.predictions, axis=1)\n",
    "            elif output_mode == \"regression\":\n",
    "                preds = np.squeeze(p.predictions)\n",
    "            return glue_compute_metrics(task_name, preds, p.label_ids)\n",
    "\n",
    "        return compute_metrics_fn\n",
    "\n",
    "    dev_compute_metrics = build_compute_metrics_fn(\"feverindomain\")\n",
    "    test_compute_metrics = build_compute_metrics_fn(\"fevercrossdomain\")\n",
    "\n",
    "\n",
    "\n",
    "    if training_args.do_train_1student_1teacher:\n",
    "        trainer = StudentTeacherTrainer(\n",
    "            tokenizer_delex,\n",
    "            tokenizer_lex,\n",
    "            models={\"teacher\": model_teacher, \"student\": model_student},\n",
    "            args=training_args,\n",
    "            train_datasets={\"combined\": None},\n",
    "            test_dataset=test_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            eval_compute_metrics=dev_compute_metrics,\n",
    "            test_compute_metrics=test_compute_metrics\n",
    "        )\n",
    "    else:\n",
    "        trainer = Trainer(\n",
    "            tokenizer_delex,\n",
    "            tokenizer_lex,\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=None,\n",
    "            eval_dataset=eval_dataset,\n",
    "            test_dataset=test_dataset,\n",
    "            eval_compute_metrics=dev_compute_metrics,\n",
    "            test_compute_metrics=test_compute_metrics\n",
    "        )\n",
    "\n",
    "    tokenizer_to_use=None\n",
    "    use_lex=False\n",
    "    use_student_teacher=True\n",
    "    use_bert_viz_model=False\n",
    "    #best student teacher trained (aka combined) models\n",
    "    #url = 'https://osf.io/twbmu/download' # light-plasma combined trained model-this model gave 59.31 cross domain fnc score and 69.21for cross domain accuracy\n",
    "    #url = 'https://osf.io/vnyad//download' # legendary-voice-1016 combined trained model-this model gave 61.52  cross domain fnc score and  74.4 for cross domain accuracy- wandb graph name legendary-voice-1016\n",
    "    if use_student_teacher==True:\n",
    "      url = 'https://osf.io/ht9gb/download'  # celestial-sun-1042 combined trained model- githubsha 21dabe wandb_celestial_sun1042 best_cd_acc_fnc_score_71.89_61.12\n",
    "\n",
    "\n",
    "    #best  models when trained on fever lexicalized data- if using this model, dont forget to use tokenizer_lex\n",
    "    #url = 'https://osf.io/q6apm/download'  # link to one of the best lex trained model- trained_model_lex_wandbGraphNameQuietHaze806_accuracy67point5_fncscore64point5_atepoch2.bin...this gave 64.58in cross domain fnc score and 67.5 for cross domain accuracy\n",
    "    # url = 'https://osf.io/fus25/download' #trained_model_lex_sweet_water_1001_trained_model_afterepoch1_accuracy6907_fncscore6254.bin\n",
    "    if(use_lex==True):\n",
    "      print(\"found use_lex==True\")\n",
    "      url = 'https://osf.io/fp89k/download' #trained_model_lex_helpful_vortex_1002_trained_model_afterepoch1_accuracy70point21percent..bin\n",
    "\n",
    "\n",
    "\n",
    "    #url = 'https://osf.io/uspm4/download'  # link to best delex trained model-this gave 55.69 in cross domain fnc score and 54.04 for cross domain accuracy\n",
    "    # refer:https://tinyurl.com/y5dyshnh for further details regarding accuracies\n",
    "\n",
    "    model =None\n",
    "    model_path = wget.download(url)    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    if use_student_teacher==True:\n",
    "        print(\"found that if use_student_teacher==True:\")\n",
    "        model=model_student\n",
    "        tokenizer_to_use=tokenizer_delex\n",
    "        print(\"going to tokenizer_delex\")\n",
    "        \n",
    "        sys.exit(1)\n",
    "\n",
    "    if use_lex==True:        \n",
    "        tokenizer_to_use=tokenizer_lex\n",
    "        model=model_teacher\n",
    "\n",
    "    \n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    #model.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "    #if using the default model of bertviz\n",
    "    if (use_bert_viz_model==True):\n",
    "      model_version = 'bert-base-cased'\n",
    "      model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "      tokenizer_to_use = BertTokenizer.from_pretrained(model_version, do_lower_case=False)\n",
    "\n",
    "    \n",
    "    sentence_a = \"Soon Marijuana May Lead to Ticket , Not Arrest , in New York\"\n",
    "    sentence_b = \"After campaigning on a promise to reform stop-and-frisk , Mayor Bill de Blasio is set to launch his most significant effort to address the issues raised by the policy .\"\n",
    "\n",
    "    if use_student_teacher==True:\n",
    "      sentence_a=\"Soon Marijuana May Lead to Ticket , Not Arrest , in governmentC1 \"\n",
    "      sentence_b = \"After campaigning on a promise to reform stop-and-frisk , Mayor politicianE1 is set to launch his most significant effort to address the issues raised by the policy .\"\n",
    "\n",
    "    assert model is not None\n",
    "    assert url is not None\n",
    "    assert len(url)>0\n",
    "    assert tokenizer_to_use is not None\n",
    "    assert len(sentence_a)>0\n",
    "    assert len(sentence_b)>0\n",
    "\n",
    "    \n",
    "    inputs = tokenizer_to_use.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "    input_id_list = input_ids[0].tolist()  # Batch index 0\n",
    "    tokens = tokenizer_to_use.convert_ids_to_tokens(input_id_list)\n",
    "    call_html()\n",
    "    head_view(attention, tokens)\n",
    "    \n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    configs = read_and_merge_config_entries()\n",
    "    print(f\"value of configs is {configs}\")\n",
    "    configs_split = configs.split()\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=configs_split)\n",
    "    training_args.output_dir=training_args.output_dir.replace(\"%20\",\" \")\n",
    "    data_args.data_dir=data_args.data_dir.replace(\"%20\",\" \")\n",
    "    training_args.toy_data_dir_path=training_args.toy_data_dir_path.replace(\"%20\",\" \")\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    run_loading_and_testing(model_args, data_args, training_args)\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    main()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ni_6r5DZKkQB"
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCaf2N-JKkQE"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x48jRaFiKkQH"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "name": "load_trained_model_visualize.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
